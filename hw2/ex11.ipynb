{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:21.472424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 17:56:21.784440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-01 17:56:21.784483: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-01 17:56:21.845262: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-01 17:56:23.910044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 17:56:23.910217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-01 17:56:23.910246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"go\", \"stop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:26.652577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-01 17:56:26.652613: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-01 17:56:26.652644: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sfanigliulo): /proc/driver/nvidia/version does not exist\n",
      "2022-12-01 17:56:26.653055: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.list_files([\"msc-train/go*\", \"msc-train/stop*\"])\n",
    "test_ds = tf.data.Dataset.list_files([\"msc-test/go*\", \"msc-test/stop*\"])\n",
    "val_ds = tf.data.Dataset.list_files([\"msc-val/go*\", \"msc-val/stop*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters-to be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_ARGS = {\n",
    "    'downsampling_rate': 16000,\n",
    "    'frame_length_in_s': 0.04,\n",
    "    'frame_step_in_s': 0.02,\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    'batch_size': 20,\n",
    "    'initial_learning_rate': 0.01,\n",
    "    'end_learning_rate': 1.e-5,\n",
    "    'epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:27.886404: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:28.860250: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:28.865848: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:28.866433: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    }
   ],
   "source": [
    "def get_audio_and_label(filename):\n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary) \n",
    "\n",
    "    path_parts = tf.strings.split(filename, '/')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '_')\n",
    "    label = file_parts[0]\n",
    "\n",
    "    audio = tf.squeeze(audio)\n",
    "    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "    audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "    return audio_padded, sampling_rate, label\n",
    "\n",
    "\n",
    "def get_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s):\n",
    "    # TODO: Write your code here\n",
    "    audio_padded, sampling_rate, label = get_audio_and_label(filename)\n",
    "    \n",
    "    if downsampling_rate != sampling_rate:\n",
    "        sampling_rate_int64 = tf.cast(sampling_rate, tf.int64)\n",
    "        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n",
    "\n",
    "    sampling_rate_float32 = tf.cast(downsampling_rate, tf.float32)\n",
    "    frame_length = int(frame_length_in_s * sampling_rate_float32)\n",
    "    frame_step = int(frame_step_in_s * sampling_rate_float32)\n",
    "\n",
    "    spectrogram = stft = tf.signal.stft(\n",
    "        audio_padded, \n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    spectrogram = tf.abs(stft)\n",
    "\n",
    "    return spectrogram, downsampling_rate, label\n",
    "\n",
    "def get_spectrogram_and_label(filename, downsampling_rate, frame_length_in_s, frame_step_in_s):\n",
    "    spectrogram, sampling_rate, label = get_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s)\n",
    "    \n",
    "    return spectrogram, label\n",
    "\n",
    "get_frozen_spectrogram = partial(get_spectrogram_and_label, **PREPROCESSING_ARGS)\n",
    "\n",
    "for spectrogram, label in train_ds.map(get_frozen_spectrogram).take(1):\n",
    "    SHAPE = spectrogram.shape\n",
    "\n",
    "def preprocess(filename):\n",
    "    signal, label = get_frozen_spectrogram(filename)\n",
    "\n",
    "    signal.set_shape(SHAPE)\n",
    "    signal = tf.expand_dims(signal, -1)\n",
    "    signal = tf.image.resize(signal, [32, 32])\n",
    "\n",
    "    label_id = tf.argmax(label == LABELS)\n",
    "\n",
    "    return signal, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:30.704257: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:30.712506: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:30.713463: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:31.570432: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:31.577060: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:31.577502: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:32.379741: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:32.389612: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-01 17:56:32.390060: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    }
   ],
   "source": [
    "batch_size = TRAINING_ARGS['batch_size']\n",
    "epochs = TRAINING_ARGS['epochs']\n",
    "\n",
    "train_ds = train_ds.map(preprocess).batch(batch_size).cache()\n",
    "val_ds = val_ds.map(preprocess).batch(batch_size)\n",
    "test_ds = test_ds.map(preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (20, 32, 32, 1)\n",
      "Data Shape: (32, 32, 1)\n",
      "Labels: tf.Tensor([1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1], shape=(20,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:56:33.299274: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 40s 475ms/step - loss: 0.5824 - sparse_categorical_accuracy: 0.7006 - val_loss: 0.5072 - val_sparse_categorical_accuracy: 0.7950\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 28s 356ms/step - loss: 0.4082 - sparse_categorical_accuracy: 0.8244 - val_loss: 0.4569 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 0.3796 - sparse_categorical_accuracy: 0.8313 - val_loss: 0.4193 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 23s 285ms/step - loss: 0.3570 - sparse_categorical_accuracy: 0.8425 - val_loss: 0.4699 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 21s 259ms/step - loss: 0.3161 - sparse_categorical_accuracy: 0.8706 - val_loss: 1.9959 - val_sparse_categorical_accuracy: 0.5300\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 19s 241ms/step - loss: 0.2791 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.3406 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 21s 267ms/step - loss: 0.2368 - sparse_categorical_accuracy: 0.9112 - val_loss: 1.7952 - val_sparse_categorical_accuracy: 0.5550\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 22s 280ms/step - loss: 0.2035 - sparse_categorical_accuracy: 0.9231 - val_loss: 0.6416 - val_sparse_categorical_accuracy: 0.7250\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 21s 258ms/step - loss: 0.1874 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.7500 - val_sparse_categorical_accuracy: 0.7250\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 21s 263ms/step - loss: 0.1587 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "initial_learning_rate = TRAINING_ARGS['initial_learning_rate']\n",
    "end_learning_rate = TRAINING_ARGS['end_learning_rate']\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=end_learning_rate,\n",
    "    decay_steps=len(train_ds) * epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 173ms/step - loss: 0.2262 - sparse_categorical_accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1587\n",
      "Training Accuracy: 94.25%\n",
      "\n",
      "Validation Loss: 0.3270\n",
      "Validation Accuracy: 90.00%\n",
      "\n",
      "Test Loss: 0.2262\n",
      "Test Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/1669914094/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/1669914094/assets\n"
     ]
    }
   ],
   "source": [
    "timestamp = int(time())\n",
    "\n",
    "saved_model_dir = f'./saved_models/{timestamp}'\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "model.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## NON RUNNARE - PER SALVARE gli iperparametri e i risultati\n",
    "# import pandas as pd\n",
    "\n",
    "# output_dict = {\n",
    "#     'timestamp': timestamp,\n",
    "#     **PREPROCESSING_ARGS,\n",
    "#     **TRAINING_ARGS,\n",
    "#     'test_accuracy': test_accuracy\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame([output_dict])\n",
    "\n",
    "# output_path='./spectrogram_results.csv'\n",
    "# df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 18:01:39.446336: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-12-01 18:01:39.446391: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-12-01 18:01:39.447678: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1669914094\n",
      "2022-12-01 18:01:39.452899: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-01 18:01:39.452946: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/1669914094\n",
      "2022-12-01 18:01:39.468105: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-12-01 18:01:39.473404: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-01 18:01:39.676985: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/1669914094\n",
      "2022-12-01 18:01:39.720291: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 272626 microseconds.\n",
      "2022-12-01 18:01:39.823522: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = timestamp\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tflite_models_dir = './tflite_models'\n",
    "if not os.path.exists(tflite_models_dir):\n",
    "    os.makedirs(tflite_models_dir)\n",
    "tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\n",
    "with open(tflite_model_name, 'wb') as fp:\n",
    "    fp.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with log mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_ARGS = {\n",
    "    'downsampling_rate': 16000,\n",
    "    'frame_length_in_s': 0.04,\n",
    "    'frame_step_in_s': 0.02,\n",
    "    'num_mel_bins': 40,\n",
    "    'lower_frequency': 20,\n",
    "    'upper_frequency': 4000,\n",
    "}\n",
    "\n",
    "LABELS = [\"go\", \"stop\"]\n",
    "\n",
    "downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n",
    "sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n",
    "frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n",
    "frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n",
    "spectrogram_width = (16000 - frame_length) // frame_step + 1\n",
    "num_spectrogram_bins = frame_length // 2 + 1\n",
    "\n",
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "    PREPROCESSING_ARGS['num_mel_bins'],\n",
    "    num_spectrogram_bins,\n",
    "    downsampling_rate,\n",
    "    PREPROCESSING_ARGS['lower_frequency'],\n",
    "    PREPROCESSING_ARGS['upper_frequency']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 1\n",
      "Number of outputs: 1\n",
      "Input name: serving_default_input_1:0\n",
      "Input shape: [ 1 32 32  1]\n",
      "Output name: StatefulPartitionedCall:0\n",
      "Output shape: [1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=f'tflite_models/{MODEL_NAME}.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Number of inputs:\", len(input_details))\n",
    "print(\"Number of outputs:\", len(output_details))\n",
    "print(\"Input name:\", input_details[0]['name'])\n",
    "print(\"Input shape:\", input_details[0]['shape'])\n",
    "print(\"Output name:\", output_details[0]['name'])\n",
    "print(\"Output shape:\", output_details[0]['shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 49 but expected 32 for dimension 1 of input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m end_preprocess \u001b[39m=\u001b[39m time()\n\u001b[1;32m     40\u001b[0m \u001b[39m#log_mel_spectrogram ha shape (1,49,40,1), ma dovrebbe essere (1,32,...)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m interpreter\u001b[39m.\u001b[39;49mset_tensor(input_details[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m], log_mel_spectrogram) \n\u001b[1;32m     42\u001b[0m interpreter\u001b[39m.\u001b[39minvoke()\n\u001b[1;32m     43\u001b[0m output \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_tensor(output_details[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_tensor\u001b[39m(\u001b[39mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    682\u001b[0m   \u001b[39m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[1;32m    684\u001b[0m \u001b[39m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mSetTensor(tensor_index, value)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 49 but expected 32 for dimension 1 of input 0."
     ]
    }
   ],
   "source": [
    "filenames = glob('msc-test/go*') + glob('msc-test/stop*')\n",
    "\n",
    "\n",
    "avg_preprocessing_latency = 0.0\n",
    "avg_model_latency = 0.0\n",
    "latencies = []\n",
    "accuracy = 0.0\n",
    "\n",
    "for filename in filenames:\n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "    path_parts = tf.strings.split(filename, '/')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '_')\n",
    "    true_label = file_parts[0]\n",
    "    true_label = true_label.numpy().decode()\n",
    "    \n",
    "    start_preprocess = time()\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary) \n",
    "    audio = tf.squeeze(audio)\n",
    "    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "    audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "    if downsampling_rate != sampling_rate:\n",
    "        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n",
    "\n",
    "    stft = tf.signal.stft(\n",
    "        audio_padded, \n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    spectrogram = tf.abs(stft)\n",
    "\n",
    "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n",
    "    end_preprocess = time()\n",
    "    \n",
    "    #log_mel_spectrogram ha shape (1,49,40,1), ma dovrebbe essere (1,32,32,1)\n",
    "    interpreter.set_tensor(input_details[0]['index'], log_mel_spectrogram) \n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    end_inference = time()\n",
    "\n",
    "    top_index = np.argmax(output[0])\n",
    "    predicted_label = LABELS[top_index]\n",
    "\n",
    "    accuracy += true_label == predicted_label\n",
    "    avg_preprocessing_latency += end_preprocess - start_preprocess\n",
    "    avg_model_latency += end_inference - end_preprocess\n",
    "    latencies.append(end_inference - start_preprocess) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(filenames)\n",
    "avg_preprocessing_latency = len(filenames)\n",
    "avg_model_latency = len(filenames)\n",
    "median_total_latency = np.median(latencies)\n",
    "\n",
    "import os\n",
    "\n",
    "model_size = os.path.getsize(f'tflite_models/{MODEL_NAME}.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {100 * accuracy:.3f}%')\n",
    "print(f'Model size: {model_size / 2 ** 10:.1f}KB')\n",
    "print(f'Preprocessing Latency: {1000 * avg_preprocessing_latency:.1f}ms')\n",
    "print(f'Model Latency: {1000 * avg_model_latency:.1f}ms')\n",
    "print(f'Total Latency: {1000 * median_total_latency:.1f}ms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
