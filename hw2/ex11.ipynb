{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/sfanigliulo/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: packaging in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (22.11.23)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/sfanigliulo/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/sfanigliulo/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: tensorflow_io in /home/sfanigliulo/.local/lib/python3.8/site-packages (0.28.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.28.0 in /home/sfanigliulo/.local/lib/python3.8/site-packages (from tensorflow_io) (0.28.0)\n",
      "Requirement already satisfied: numpy in /home/sfanigliulo/.local/lib/python3.8/site-packages (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tensorflow_io\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"go\", \"stop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.list_files([\"msc-train/go*\", \"msc-train/stop*\"])\n",
    "test_ds = tf.data.Dataset.list_files([\"msc-test/go*\", \"msc-test/stop*\"])\n",
    "val_ds = tf.data.Dataset.list_files([\"msc-val/go*\", \"msc-val/stop*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters-to be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_ARGS = {\n",
    "    'downsampling_rate': 16000,\n",
    "    'frame_length_in_s': 0.04,\n",
    "    'frame_step_in_s': 0.02,\n",
    "    # AGGIUNGI ROBA\n",
    "}\n",
    "\n",
    "# PREPROCESSING_ARGS = {\n",
    "#     'downsampling_rate': 16000,\n",
    "#     'frame_length_in_s': 0.04,\n",
    "#     'frame_step_in_s': 0.02,\n",
    "#     'num_mel_bins': 40,\n",
    "#     'lower_frequency': 20,\n",
    "#     'upper_frequency': 4000,\n",
    "# }\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    'batch_size': 20,\n",
    "    'initial_learning_rate': 0.01,\n",
    "    'end_learning_rate': 1.e-5,\n",
    "    'epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "2022-12-06 16:54:30.234405: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:30.235556: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:30.235677: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    }
   ],
   "source": [
    "def get_audio_and_label(filename):\n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary) \n",
    "\n",
    "    path_parts = tf.strings.split(filename, '/')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '_')\n",
    "    label = file_parts[0]\n",
    "\n",
    "    audio = tf.squeeze(audio)\n",
    "    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "    audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "    return audio_padded, sampling_rate, label\n",
    "\n",
    "\n",
    "def get_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s):\n",
    "    audio_padded, sampling_rate, label = get_audio_and_label(filename)\n",
    "    \n",
    "    if downsampling_rate != sampling_rate:\n",
    "        sampling_rate_int64 = tf.cast(sampling_rate, tf.int64)\n",
    "        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n",
    "\n",
    "    sampling_rate_float32 = tf.cast(downsampling_rate, tf.float32)\n",
    "    frame_length = int(frame_length_in_s * sampling_rate_float32)\n",
    "    frame_step = int(frame_step_in_s * sampling_rate_float32)\n",
    "\n",
    "    spectrogram = stft = tf.signal.stft(\n",
    "        audio_padded, \n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    spectrogram = tf.abs(stft)\n",
    "\n",
    "    return spectrogram, downsampling_rate, label\n",
    "\n",
    "def get_spectrogram_and_label(filename, downsampling_rate, frame_length_in_s, frame_step_in_s):\n",
    "    spectrogram, sampling_rate, label = get_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s)\n",
    "    \n",
    "    return spectrogram, label\n",
    "\n",
    "get_frozen_spectrogram = partial(get_spectrogram_and_label, **PREPROCESSING_ARGS)\n",
    "\n",
    "for spectrogram, label in train_ds.map(get_frozen_spectrogram).take(1):\n",
    "    SHAPE = spectrogram.shape\n",
    "\n",
    "def preprocess(filename):\n",
    "    signal, label = get_frozen_spectrogram(filename)\n",
    "\n",
    "    signal.set_shape(SHAPE)\n",
    "    signal = tf.expand_dims(signal, -1)\n",
    "    signal = tf.image.resize(signal, [32, 32])\n",
    "\n",
    "    label_id = tf.argmax(label == LABELS)\n",
    "\n",
    "    return signal, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_audio_and_label(filename):\n",
    "#     audio_binary = tf.io.read_file(filename)\n",
    "#     audio, sampling_rate = tf.audio.decode_wav(audio_binary) \n",
    "\n",
    "#     path_parts = tf.strings.split(filename, '/')\n",
    "#     path_end = path_parts[-1]\n",
    "#     file_parts = tf.strings.split(path_end, '_')\n",
    "#     label = file_parts[0]\n",
    "\n",
    "#     audio = tf.squeeze(audio)\n",
    "#     zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "#     audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "#     return audio_padded, sampling_rate, label\n",
    "\n",
    "# def get_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s):\n",
    "#     audio_padded, sampling_rate, label = get_audio_and_label(filename)\n",
    "    \n",
    "#     if downsampling_rate != sampling_rate:\n",
    "#         sampling_rate_int64 = tf.cast(sampling_rate, tf.int64)\n",
    "#         audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n",
    "\n",
    "#     sampling_rate_float32 = tf.cast(downsampling_rate, tf.float32)\n",
    "#     frame_length = int(frame_length_in_s * sampling_rate_float32)\n",
    "#     frame_step = int(frame_step_in_s * sampling_rate_float32)\n",
    "\n",
    "#     spectrogram = stft = tf.signal.stft(\n",
    "#         audio_padded, \n",
    "#         frame_length=frame_length,\n",
    "#         frame_step=frame_step,\n",
    "#         fft_length=frame_length\n",
    "#     )\n",
    "#     spectrogram = tf.abs(stft)\n",
    "\n",
    "#     return spectrogram, downsampling_rate, label\n",
    "\n",
    "# def get_log_mel_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency):\n",
    "#     # TODO: Write your code here\n",
    "#     spectrogram, sampling_rate, label = get_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s)\n",
    "\n",
    "#     sampling_rate_float32 = tf.cast(sampling_rate, tf.float32)\n",
    "#     frame_length = int(frame_length_in_s * sampling_rate_float32)\n",
    "#     num_spectrogram_bins = frame_length // 2 + 1\n",
    "\n",
    "#     linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "#         num_mel_bins=num_mel_bins,\n",
    "#         num_spectrogram_bins=num_spectrogram_bins,\n",
    "#         sample_rate=sampling_rate,\n",
    "#         lower_edge_hertz=lower_frequency,\n",
    "#         upper_edge_hertz=upper_frequency\n",
    "#     )\n",
    "\n",
    "#     mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "\n",
    "#     log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "\n",
    "#     return log_mel_spectrogram, label\n",
    "\n",
    "# def get_log_mel_spectrogram_and_label(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency):\n",
    "#     log_mel_spectrogram, label = get_log_mel_spectrogram(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency)\n",
    "    \n",
    "#     return log_mel_spectrogram, label\n",
    "\n",
    "# get_frozen_log_mel_spectrogram = partial(get_log_mel_spectrogram_and_label, **PREPROCESSING_ARGS)\n",
    "\n",
    "# for log_mel_spectrogram, label in train_ds.map(get_frozen_log_mel_spectrogram).take(1):\n",
    "#     SHAPE = log_mel_spectrogram.shape\n",
    "\n",
    "\n",
    "# def preprocess_with_mel(filename):\n",
    "#     signal, label = get_frozen_log_mel_spectrogram(filename)\n",
    "#     signal = tf.expand_dims(signal, -1)\n",
    "#     label_id = tf.argmax(label == LABELS)\n",
    "\n",
    "#     return signal, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 16:54:30.673996: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:30.675117: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:30.675241: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "2022-12-06 16:54:30.835777: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:30.836892: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:30.837037: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n",
      "2022-12-06 16:54:31.001201: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:31.002219: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
      "2022-12-06 16:54:31.002361: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n"
     ]
    }
   ],
   "source": [
    "batch_size = TRAINING_ARGS['batch_size']\n",
    "epochs = TRAINING_ARGS['epochs']\n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(preprocess).batch(batch_size).cache()\n",
    "val_ds = val_ds.map(preprocess).batch(batch_size)\n",
    "test_ds = test_ds.map(preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_with_resized_mel(filename):\n",
    "#     signal, label = get_frozen_log_mel_spectrogram(filename)\n",
    "#     signal.set_shape(SHAPE)\n",
    "#     signal = tf.expand_dims(signal, -1)\n",
    "#     signal = tf.image.resize(signal, [32, 32])\n",
    "#     label_id = tf.argmax(label == LABELS)\n",
    "\n",
    "#     return signal, label_id\n",
    "\n",
    "# train_ds = train_ds.map(preprocess_with_resized_mel).batch(batch_size).cache()\n",
    "# val_ds = val_ds.map(preprocess_with_resized_mel).batch(batch_size)\n",
    "# test_ds = test_ds.map(preprocess_with_resized_mel).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (20, 32, 32, 1)\n",
      "Data Shape: (32, 32, 1)\n",
      "Labels: tf.Tensor([0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1], shape=(20,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 16:54:31.347607: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 7s 76ms/step - loss: 0.5124 - sparse_categorical_accuracy: 0.7450 - val_loss: 2.1112 - val_sparse_categorical_accuracy: 0.4950\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 3s 42ms/step - loss: 0.4229 - sparse_categorical_accuracy: 0.8188 - val_loss: 0.7194 - val_sparse_categorical_accuracy: 0.6400\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 3s 42ms/step - loss: 0.3568 - sparse_categorical_accuracy: 0.8462 - val_loss: 0.6746 - val_sparse_categorical_accuracy: 0.6650\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 3s 42ms/step - loss: 0.3167 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.6762 - val_sparse_categorical_accuracy: 0.7000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 3s 41ms/step - loss: 0.2940 - sparse_categorical_accuracy: 0.8813 - val_loss: 0.4700 - val_sparse_categorical_accuracy: 0.8300\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 4s 45ms/step - loss: 0.2613 - sparse_categorical_accuracy: 0.8931 - val_loss: 0.4463 - val_sparse_categorical_accuracy: 0.8050\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 3s 41ms/step - loss: 0.2330 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.3366 - val_sparse_categorical_accuracy: 0.8900\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 3s 41ms/step - loss: 0.2216 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.3253 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 3s 42ms/step - loss: 0.1999 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.3193 - val_sparse_categorical_accuracy: 0.8900\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 3s 42ms/step - loss: 0.1764 - sparse_categorical_accuracy: 0.9331 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "initial_learning_rate = TRAINING_ARGS['initial_learning_rate']\n",
    "end_learning_rate = TRAINING_ARGS['end_learning_rate']\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=end_learning_rate,\n",
    "    decay_steps=len(train_ds) * epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 35ms/step - loss: 0.2138 - sparse_categorical_accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1764\n",
      "Training Accuracy: 93.31%\n",
      "\n",
      "Validation Loss: 0.3271\n",
      "Validation Accuracy: 88.50%\n",
      "\n",
      "Test Loss: 0.2138\n",
      "Test Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/1670342109/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/1670342109/assets\n"
     ]
    }
   ],
   "source": [
    "timestamp = int(time())\n",
    "\n",
    "saved_model_dir = f'./saved_models/{timestamp}'\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "model.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## NON RUNNARE - PER SALVARE gli iperparametri e i risultati\n",
    "# import pandas as pd\n",
    "\n",
    "# output_dict = {\n",
    "#     'timestamp': timestamp,\n",
    "#     **PREPROCESSING_ARGS,\n",
    "#     **TRAINING_ARGS,\n",
    "#     'test_accuracy': test_accuracy\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame([output_dict])\n",
    "\n",
    "# output_path='./spectrogram_results.csv'\n",
    "# df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 16:55:11.206135: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-12-06 16:55:11.206182: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-12-06 16:55:11.206400: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1670342109\n",
      "2022-12-06 16:55:11.211108: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-06 16:55:11.211152: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/1670342109\n",
      "2022-12-06 16:55:11.223310: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-06 16:55:11.331527: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/1670342109\n",
      "2022-12-06 16:55:11.356691: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 150291 microseconds.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = timestamp\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tflite_models_dir = './tflite_models'\n",
    "if not os.path.exists(tflite_models_dir):\n",
    "    os.makedirs(tflite_models_dir)\n",
    "tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\n",
    "with open(tflite_model_name, 'wb') as fp:\n",
    "    fp.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with log mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_ARGS = {\n",
    "    'downsampling_rate': 16000,\n",
    "    'frame_length_in_s': 0.04,\n",
    "    'frame_step_in_s': 0.02,\n",
    "    'num_mel_bins': 40,\n",
    "    'lower_frequency': 20,\n",
    "    'upper_frequency': 4000,\n",
    "}\n",
    "\n",
    "LABELS = [\"go\", \"stop\"]\n",
    "\n",
    "downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n",
    "sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n",
    "frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n",
    "frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n",
    "spectrogram_width = (16000 - frame_length) // frame_step + 1\n",
    "num_spectrogram_bins = frame_length // 2 + 1\n",
    "\n",
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "    PREPROCESSING_ARGS['num_mel_bins'],\n",
    "    num_spectrogram_bins,\n",
    "    downsampling_rate,\n",
    "    PREPROCESSING_ARGS['lower_frequency'],\n",
    "    PREPROCESSING_ARGS['upper_frequency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DA RUNNARE SOLO CON LA COMBINAZIONE GIUSTA DA PASSARE AL EX12\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame([PREPROCESSING_ARGS])\n",
    "# df.to_csv(\"spectrogram_results.csv\", mode='a', header=not os.path.exists(\"spectrogram_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs: 1\n",
      "Number of outputs: 1\n",
      "Input name: serving_default_input_4:0\n",
      "Input shape: [ 1 32 32  1]\n",
      "Output name: StatefulPartitionedCall:0\n",
      "Output shape: [1 2]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=f'tflite_models/{MODEL_NAME}.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Number of inputs:\", len(input_details))\n",
    "print(\"Number of outputs:\", len(output_details))\n",
    "print(\"Input name:\", input_details[0]['name'])\n",
    "print(\"Input shape:\", input_details[0]['shape'])\n",
    "print(\"Output name:\", output_details[0]['name'])\n",
    "print(\"Output shape:\", output_details[0]['shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('msc-test/go*') + glob('msc-test/stop*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.data.Dataset.list_files(filenames)\n",
    "# filenames = filenames.map(preprocess_with_resized_mel).batch(batch_size).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preprocessing_latency = 0.0\n",
    "avg_model_latency = 0.0\n",
    "latencies = []\n",
    "accuracy = 0.0\n",
    "\n",
    "for filename in filenames:\n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "    path_parts = tf.strings.split(filename, '/')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '_')\n",
    "    true_label = file_parts[0]\n",
    "    true_label = true_label.numpy().decode()\n",
    "    \n",
    "    start_preprocess = time()\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary) \n",
    "    audio = tf.squeeze(audio)\n",
    "    zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n",
    "    audio_padded = tf.concat([audio, zero_padding], axis=0)\n",
    "\n",
    "    if downsampling_rate != sampling_rate:\n",
    "        audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n",
    "\n",
    "    stft = tf.signal.stft(\n",
    "        audio_padded, \n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    #print(stft.shape)  ###\n",
    "    spectrogram = tf.abs(stft)\n",
    "    #print(spectrogram.shape)   ###\n",
    "    #print(linear_to_mel_weight_matrix.shape)  ###\n",
    "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "    #print(mel_spectrogram.shape)   ###\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n",
    "    log_mel_spectrogram = tf.image.resize(log_mel_spectrogram, [32, 32])\n",
    "    end_preprocess = time()\n",
    "    \n",
    "    #log_mel_spectrogram ha shape (1,49,40,1), ma dovrebbe essere (1,32,32,1)\n",
    "    interpreter.set_tensor(input_details[0]['index'], log_mel_spectrogram) \n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    end_inference = time()\n",
    "\n",
    "    top_index = np.argmax(output[0])\n",
    "    predicted_label = LABELS[top_index]\n",
    "\n",
    "    accuracy += true_label == predicted_label\n",
    "    avg_preprocessing_latency += end_preprocess - start_preprocess\n",
    "    avg_model_latency += end_inference - end_preprocess\n",
    "    latencies.append(end_inference - start_preprocess) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy /= len(filenames)\n",
    "avg_preprocessing_latency /= len(filenames)\n",
    "avg_model_latency /= len(filenames)\n",
    "median_total_latency = np.median(latencies)\n",
    "\n",
    "import os\n",
    "\n",
    "model_size = os.path.getsize(f'tflite_models/{MODEL_NAME}.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.500%\n",
      "Model size: 1162.1KB\n",
      "Preprocessing Latency: 14.4ms\n",
      "Model Latency: 2.0ms\n",
      "Total Latency: 16.6ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {100 * accuracy:.3f}%')\n",
    "print(f'Model size: {model_size / 2 ** 10:.1f}KB')\n",
    "print(f'Preprocessing Latency: {1000 * avg_preprocessing_latency:.1f}ms')\n",
    "print(f'Model Latency: {1000 * avg_model_latency:.1f}ms')\n",
    "print(f'Total Latency: {1000 * median_total_latency:.1f}ms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
